{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbd5da8",
   "metadata": {},
   "source": [
    "## Detector de **SPAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbc640",
   "metadata": {},
   "source": [
    "### Objetivo do projeto\n",
    "\n",
    "Este projeto tem o objetivo de mostrar como o **Multilayer Perceptron** pode ser utilizado para a detecção de SPAM. Embora o projeto esteja em Língua Portuguesa do Brasil, a base de SMS utilizada foi [encontrada no Kaggle](https://www.kaggle.com/code/dhgupta/bag-of-words-model/input), em Língua Inglesa, com o original disponibilizado em `data/spam.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb834e95",
   "metadata": {},
   "source": [
    "### Análise dos dados\n",
    "\n",
    "O primeiro passo em um projeto dessa natureza é analisar os dados. Para tanto, faremos uma leitura da base original e vamos contar o número palavras e frases. Ao final dessa tarefa, vamos decidir por executar ou não algum pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dc89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalar pacotes necessários\n",
    "%pip install -q pandas matplotlib seaborn wordcloud nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bbb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "import pandas as pd                    # Leitura e manipulação de dados em DataFrames\n",
    "import matplotlib.pyplot as plt        # Gráficos e visualizações\n",
    "import seaborn as sns                  # Gráficos estatísticos com uma estética aprimorada\n",
    "from wordcloud import WordCloud        # Nuvens de palavras\n",
    "import nltk                            # Processamento de linguagem natural (NLP)\n",
    "from nltk.corpus import stopwords      # Lstas de stopwords (palavras irrelevantes)\n",
    "from collections import Counter        # Frequência de elementos\n",
    "import string                          # Manipulação de pontuação e caracteres especiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceab9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape da base SPAM original: (5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                sms\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Leitura do arquivo\n",
    "spam = pd.read_csv(\"data/spam.csv\", encoding='utf-8')   # Codificação para evitar erros\n",
    "spam = spam[['Label', 'EmailText']]                     # manter apenas colunas relevantes\n",
    "spam.columns = ['label', 'sms']                         # renomeia para dar melhor sentido\n",
    "print(f'Linhas da base SPAM original: {spam.shape}')    # Imprime dimensão da base\n",
    "spam.head()                                             # Imprime cinco registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda41c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Estatísticas básicas\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "\n",
    "print(\"Mensagens por categoria:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\nMédia de palavras por mensagem:\")\n",
    "print(df.groupby('label')['word_count'].mean())\n",
    "\n",
    "print(\"\\nTamanho da maior mensagem (em caracteres):\")\n",
    "print(df['char_count'].max())\n",
    "\n",
    "print(\"\\nMensagem mais longa:\")\n",
    "print(df.loc[df['char_count'].idxmax(), 'text'])\n",
    "\n",
    "# 5. Visualização: histograma do comprimento das mensagens\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=df, x='char_count', hue='label', bins=50, kde=True)\n",
    "plt.title(\"Distribuição do tamanho das mensagens\")\n",
    "plt.xlabel(\"Número de caracteres\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Nuvem de palavras (WordCloud)\n",
    "def generate_wordcloud(text, title):\n",
    "    wc = WordCloud(width=800, height=400, background_color='white', stopwords='english').generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Juntar textos de cada categoria\n",
    "spam_text = \" \".join(df[df['label'] == 'spam']['text'])\n",
    "ham_text = \" \".join(df[df['label'] == 'ham']['text'])\n",
    "\n",
    "generate_wordcloud(spam_text, \"Nuvem de Palavras - SPAM\")\n",
    "generate_wordcloud(ham_text, \"Nuvem de Palavras - HAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af451e",
   "metadata": {},
   "source": [
    "#### Análise de Frequência de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Baixar recursos do nltk (apenas na 1ª vez)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Função de limpeza e tokenização\n",
    "def get_word_frequencies(text_series):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_words = []\n",
    "    for message in text_series:\n",
    "        # Tokenizar\n",
    "        words = nltk.word_tokenize(message.lower())\n",
    "        # Remover pontuações e stopwords\n",
    "        words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words)\n",
    "\n",
    "# Frequências de palavras\n",
    "spam_freq = get_word_frequencies(df[df['label'] == 'spam']['text'])\n",
    "ham_freq = get_word_frequencies(df[df['label'] == 'ham']['text'])\n",
    "\n",
    "# Mostrar 20 palavras mais comuns\n",
    "print(\"Top 20 palavras em SPAM:\")\n",
    "print(spam_freq.most_common(20))\n",
    "\n",
    "print(\"\\nTop 20 palavras em HAM:\")\n",
    "print(ham_freq.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13fe82",
   "metadata": {},
   "source": [
    "####  Palavras mais comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para DataFrame e plotar\n",
    "def plot_common_words(word_counter, title, n=20):\n",
    "    common_words = word_counter.most_common(n)\n",
    "    words, counts = zip(*common_words)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=list(counts), y=list(words), palette=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frequência\")\n",
    "    plt.ylabel(\"Palavra\")\n",
    "    plt.show()\n",
    "\n",
    "plot_common_words(spam_freq, \"Palavras Mais Comuns em SPAM\")\n",
    "plot_common_words(ham_freq, \"Palavras Mais Comuns em HAM\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
