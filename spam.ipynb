{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbd5da8",
   "metadata": {},
   "source": [
    "## Detector de **SPAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbc640",
   "metadata": {},
   "source": [
    "### Objetivo do projeto\n",
    "\n",
    "Este projeto tem o objetivo de mostrar como o **Multilayer Perceptron** pode ser utilizado para a detecção de SPAM. Embora o projeto esteja em Língua Portuguesa do Brasil, a base de SMS utilizada foi [encontrada no Kaggle](https://www.kaggle.com/code/dhgupta/bag-of-words-model/input), em Língua Inglesa, com o original disponibilizado em `data/spam.csv`.\n",
    "\n",
    "Muitos projetos para a avaliação de SPAM em Língua Inglesa existem, assim deseja-se que este projeto faça tal classificação em Língua Portuguesa sendo necessário, para tanto, a tradução de toda a base de dados que será feita da seguinte forma:\n",
    "\n",
    "1. Utilização de *small language models* no [Ollama](https://ollama.com) para rodar modelos de linguagem local;\n",
    "1. Utilização do modelo [Improve Gramar](https://ollama.com/gnokit/improve-grammar) para a correção gramatical do texto na língua original, gerando o arquivo `data/spam_en.csv`;\n",
    "1. Personalização do modelo [Gemma3 Translator](https://ollama.com/zongwei/gemma3-translator:1b) por meio do arquivo `en2pt.modelfile`, gerando o arquivo `data/spam_br.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb834e95",
   "metadata": {},
   "source": [
    "### Análise dos dados\n",
    "\n",
    "O primeiro passo em um projeto dessa natureza é analisar os dados. Para tanto, faremos uma leitura da base original e vamos contar o número palavras e frases. Ao final dessa tarefa, vamos decidir por executar ou não algum pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar pacotes necessários\n",
    "%pip install -q pandas matplotlib seaborn wordcloud nltk ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bbb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "import pandas as pd                    # Leitura e manipulação de dados em DataFrames\n",
    "import matplotlib.pyplot as plt        # Gráficos e visualizações\n",
    "import seaborn as sns                  # Gráficos estatísticos com uma estética aprimorada\n",
    "from wordcloud import WordCloud        # Nuvens de palavras\n",
    "import nltk                            # Processamento de linguagem natural (NLP)\n",
    "from nltk.corpus import stopwords      # Lstas de stopwords (palavras irrelevantes)\n",
    "from collections import Counter        # Frequência de elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando no modo debug...\n",
      "Arquivo salvo em: data/spam_en.csv\n"
     ]
    }
   ],
   "source": [
    "# Processa verificação gramática das mensagens usando Ollama\n",
    "import os\n",
    "from scripts.gramar import process_grammar\n",
    "\n",
    "input_csv = 'data/spam.csv'\n",
    "output_csv = 'data/spam_en.csv'\n",
    "\n",
    "if not os.path.exists(output_csv):\n",
    "    # Ative o debug se quiser depurar a função\n",
    "    process_grammar(input_csv, output_csv, debug=False, num_workers=4)\n",
    "else:\n",
    "    print(f'O arquivo {output_csv} já existe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceab9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Leitura do arquivo\n",
    "spam = pd.read_csv('data/spam.csv', encoding='utf-8')   # Codificação para evitar erros\n",
    "spam = spam[['Label', 'EmailText']]                     # manter apenas colunas relevantes\n",
    "spam.columns = ['label', 'sms']                         # renomeia para dar melhor sentido\n",
    "print(f'Linhas da base SPAM original: {spam.shape}')    # Imprime dimensão da base\n",
    "spam.head()                                             # Imprime cinco registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11739d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas básicas\n",
    "spam['word_count'] = spam['sms'].apply(lambda x: len(x.split()))\n",
    "spam['char_count'] = spam['sms'].apply(len)\n",
    "\n",
    "# Contar mensagens por categoria (ham/spam)\n",
    "sms_by_label = spam['label'].value_counts().to_frame(name='Número de SMS')\n",
    "sms_by_label.index.name = 'label'\n",
    "\n",
    "# Média de palavras por mensagem\n",
    "words_by_sms = spam.groupby('label')['word_count'].mean().to_frame(name='Média palavras por SMS')\n",
    "\n",
    "# Juntar os dois DataFrames\n",
    "sms_by_label.join(words_by_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb5516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maiores e menores SMSs\n",
    "\n",
    "# Ordenar as cinco maiores e menores mensagens\n",
    "maiores = spam.sort_values(by='char_count', ascending=False).head(5)\n",
    "menores = spam.sort_values(by='char_count', ascending=True).head(5)\n",
    "maiores_menores = pd.concat([maiores, menores], axis=0)\n",
    "maiores_menores[['label', 'char_count', 'word_count', 'sms']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e620c51",
   "metadata": {},
   "source": [
    "### Resultado da análise\n",
    "\n",
    "De acordo com a análise até o momento, temos um claro desbalanceamento nas classes.\n",
    "\n",
    "| Classe | Nº de amostras | Palavras por mensagem (média) |\n",
    "| ------ | -------------- | ----------------------------- |\n",
    "| ham    | 4.825          | \\~14                          |\n",
    "| spam   | 747            | \\~24                          |\n",
    "\n",
    "\n",
    "A classe `ham` representa cerca de 87% dos dados, assim um modelo treinado sem cuidado pode aprender a simplesmente prever tudo como `ham` e ainda parecer `preciso`.\n",
    "\n",
    "Foi o que aconteceu no projeto do diabetes, mas o Pedro, como bom estatístico, sempre tem uma boa explicação para os dados --- os dados dizem o que ele quer que diga. Os modelos apresentados lá, tanto a versão MLP quanto a logística, **não funcionavam**. Tentei instigar à correção, mas fui voto vencido.\n",
    "\n",
    "Nesse caso, como estou fazendo sozinho, não vou permitir tal desbalanceamento. Para corrigir vou usar uma ou mais das possibilidades à seguir:\n",
    "\n",
    "* Na hora de chamar o treinador MLP, dar peso maior para a classe 'desbalanceada':\n",
    "    ```\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(64,), max_iter=300, random_state=42, class_weight='balanced')\n",
    "    ```\n",
    "\n",
    "* Usar o `SMOTE` para fazer criar amostras sintéticas da classe `spam`\n",
    "    ```\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    ```\n",
    "\n",
    "* Remover dados da classe `ham`, mas não vou seguir essa abordagem dado que a quantidade de dados já está baixa.\n",
    "\n",
    "* Uma outra atividade que podemos fazer é a produção de novos dados com base na correção gramatical do inglês e na tradução para o português, o que seria desejável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda41c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 5. Visualização: histograma do comprimento das mensagens\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=spam, x='char_count', hue='label', bins=50, kde=True)\n",
    "plt.title('Distribuição do tamanho das mensagens')\n",
    "plt.xlabel('Número de caracteres')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()\n",
    "\n",
    "# 6. Nuvem de palavras (WordCloud)\n",
    "def generate_wordcloud(text, title):\n",
    "    wc = WordCloud(width=800, height=400, background_color='white', stopwords='english').generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Juntar textos de cada categoria\n",
    "spam_text = ' '.join(spam[spam['label'] == 'spam']['sms'])\n",
    "ham_text = ' '.join(spam[spam['label'] == 'ham']['sms'])\n",
    "\n",
    "generate_wordcloud(spam_text, 'Nuvem de Palavras - SPAM')\n",
    "generate_wordcloud(ham_text, 'Nuvem de Palavras - HAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af451e",
   "metadata": {},
   "source": [
    "#### Análise de Frequência de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Baixar recursos do nltk (apenas na 1ª vez)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Função de limpeza e tokenização\n",
    "def get_word_frequencies(text_series):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_words = []\n",
    "    for message in text_series:\n",
    "        # Tokenizar\n",
    "        words = nltk.word_tokenize(message.lower())\n",
    "        # Remover pontuações e stopwords\n",
    "        words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words)\n",
    "\n",
    "# Frequências de palavras\n",
    "spam_freq = get_word_frequencies(df[df['label'] == 'spam']['text'])\n",
    "ham_freq = get_word_frequencies(df[df['label'] == 'ham']['text'])\n",
    "\n",
    "# Mostrar 20 palavras mais comuns\n",
    "print('Top 20 palavras em SPAM:')\n",
    "print(spam_freq.most_common(20))\n",
    "\n",
    "print('\\nTop 20 palavras em HAM:')\n",
    "print(ham_freq.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13fe82",
   "metadata": {},
   "source": [
    "####  Palavras mais comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para DataFrame e plotar\n",
    "def plot_common_words(word_counter, title, n=20):\n",
    "    common_words = word_counter.most_common(n)\n",
    "    words, counts = zip(*common_words)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=list(counts), y=list(words), palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Frequência')\n",
    "    plt.ylabel('Palavra')\n",
    "    plt.show()\n",
    "\n",
    "plot_common_words(spam_freq, 'Palavras Mais Comuns em SPAM')\n",
    "plot_common_words(ham_freq, 'Palavras Mais Comuns em HAM')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
